---
title: "ML-Powered Rock-Paper-Scissors Robot"
excerpt: "Designed and implemented computer-vision-based ML models to predict a player’s next Rock-Paper-Scissors move and actuate the corresponding gesture on a robotic hand in real time. <br/><img src='/images/robotic_hand.png'>"
collection: portfolio
---

### Overview
Collaborated with a partner to engineer an **AI-driven Rock-Paper-Scissors robot** that plays interactively against a human opponent. The system integrates computer vision for gesture recognition and machine learning models for move prediction, with real-time output on a robotic hand.

### Technical Highlights
- Developed a **real-time gesture recognition pipeline** using Python, OpenCV, and MediaPipe, achieving **~90% gesture detection accuracy**.  
- Implemented and compared **three predictive models** — Conditional Probability, Markov Model, and Q-Learning — with the **Markov model improving win rate from 33% to over 50% after ~50 rounds**.  
- Integrated predictions with an **Arduino-controlled robotic hand**, enabling physical responses **within 1 second** of user input across **200+ interactive rounds**.  
- Designed end-to-end system architecture covering perception (vision), prediction (AI), and actuation (robotics).

### Presentation
[View the full presentation on Google Slides](https://docs.google.com/presentation/d/183zougUu1o3fSZLRUlkprbhu35HpOPr0kDezPrryTww/edit?usp=sharing)
